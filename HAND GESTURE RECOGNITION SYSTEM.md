# BUTLER Hand Gesture Recognition System - COMPLETE GUIDE

**Property of 5KROBOTICS & MALHAR LABADE**

Complete two-app camera system for real-time hand gesture detection with AI acceleration ready.

---

## ğŸ¥ System Overview

BUTLER uses a **two-app camera architecture**:

```
USB Camera (1280x960 @ 30fps)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaPipe Hand Detection   â”‚  â† 21-point hand landmarks
â”‚  (CPU: 10-15 FPS)           â”‚
â”‚  (Hailo: 30+ FPS) âš¡         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2 Independent Applications:             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ligament.py                          â”‚ â† Bone measurement
â”‚ 2. butler_camera_display.py             â”‚ â† Gesture visualization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”œâ”€â†’ ROS2 /cmd_vel (distance navigator)
â””â”€â†’ VNC Display (real-time visualization)
```

### Performance Comparison

| Feature | CPU Only | With Hailo-8 |
|---------|----------|-------------|
| Detection FPS | 10-15 | 30+ âš¡ |
| Hand latency | 100-150ms | 33ms âš¡ |
| CPU usage | 80%+ | 20-30% âš¡ |
| Detection range | 10 feet | 10 feet |
| Accuracy | Good | Excellent |

---

## ğŸ”Œ Hardware Setup

### Camera Configuration

```
USB Webcam
â”œâ”€â”€ Resolution: 1280x960 (optimized for hand detection)
â”œâ”€â”€ FPS: 30 (capture rate)
â”œâ”€â”€ Exposure: Auto
â””â”€â”€ Connected to: /dev/video0

MediaPipe Detection
â”œâ”€â”€ Model: Hand detection (blazepalm)
â”œâ”€â”€ Landmarks: 21 3D points per hand
â”œâ”€â”€ Confidence threshold: 0.3 (detection), 0.3 (tracking)
â””â”€â”€ Hand limit: 1 (performance optimization)

Ligament Analysis
â”œâ”€â”€ Bones detected: 15 (5 per digit + palm)
â”œâ”€â”€ Measurements: Precise 3D distances
â””â”€â”€ Buffer size: 6 frames (gesture stability)
```

### Optional: Hailo-8 GPU Acceleration

See [AI_HAT_ENHANCED.md](AI_HAT_ENHANCED.md) for installation.

```
Before:
USB Camera â†’ CPU MediaPipe (10-15 FPS) â†’ Apps

After:
USB Camera â†’ GPU MediaPipe (30+ FPS) â†’ Apps
                â†“
            Hailo-8 NPU
            (13 TOPS)
```

---

## ğŸ’» Application 1: ligament.py

**Location**: `src/butler_camera/ligament.py`  
**Purpose**: Hand measurement and ligament bone analysis  
**Platform**: ALFRIDROS (Pi5, ROS2 Jazzy)

### Source Code

```python
#!/usr/bin/env python3

import cv2
import mediapipe as mp
import numpy as np
import math
import time

class LigamentAnalyzer:
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3
        )
        self.mp_drawing = mp.solutions.drawing_utils
        
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        self.gesture_buffer = []
        self.buffer_size = 6
        self.gesture_threshold = 4
        
        self.measurement_display_time = 4.0
        self.last_measurement_time = 0
        self.last_measurements = {}
    
    def calculate_distance(self, point1, point2):
        """Calculate Euclidean distance between two 3D points"""
        return math.sqrt(
            (point1.x - point2.x)**2 + 
            (point1.y - point2.y)**2 + 
            (point1.z - point2.z)**2
        )
    
    def get_hand_size(self, landmarks):
        """Calculate overall hand size from landmarks"""
        distances = []
        
        # Thumb (2-3-4)
        distances.append(self.calculate_distance(landmarks[2], landmarks[3]))
        distances.append(self.calculate_distance(landmarks[3], landmarks[4]))
        
        # Index (5-6-7-8)
        distances.append(self.calculate_distance(landmarks[5], landmarks[6]))
        distances.append(self.calculate_distance(landmarks[6], landmarks[7]))
        distances.append(self.calculate_distance(landmarks[7], landmarks[8]))
        
        # Middle (9-10-11-12)
        distances.append(self.calculate_distance(landmarks[9], landmarks[10]))
        distances.append(self.calculate_distance(landmarks[10], landmarks[11]))
        distances.append(self.calculate_distance(landmarks[11], landmarks[12]))
        
        # Ring (13-14-15-16)
        distances.append(self.calculate_distance(landmarks[13], landmarks[14]))
        distances.append(self.calculate_distance(landmarks[14], landmarks[15]))
        distances.append(self.calculate_distance(landmarks[15], landmarks[16]))
        
        # Pinky (17-18-19-20)
        distances.append(self.calculate_distance(landmarks[17], landmarks[18]))
        distances.append(self.calculate_distance(landmarks[18], landmarks[19]))
        distances.append(self.calculate_distance(landmarks[19], landmarks[20]))
        
        return np.mean(distances)
    
    def count_extended_fingers(self, landmarks):
        """Count how many fingers are extended"""
        extended_count = 0
        
        if landmarks[4].x < landmarks[3].x:
            extended_count += 1
        
        for finger_tip in [8, 12, 16, 20]:
            finger_pip = finger_tip - 2
            if landmarks[finger_tip].y < landmarks[finger_pip].y:
                extended_count += 1
        
        return extended_count
    
    def detect_gesture(self, landmarks):
        """Detect gesture: OPEN HAND or FIST"""
        extended_fingers = self.count_extended_fingers(landmarks)
        
        if extended_fingers >= 4:
            return "OPEN"
        else:
            return "FIST"
    
    def run(self):
        """Main loop for ligament analysis"""
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_frame)
            
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(
                        frame, 
                        hand_landmarks,
                        self.mp_hands.HAND_CONNECTIONS
                    )
                    
                    gesture = self.detect_gesture(hand_landmarks.landmark)
                    hand_size = self.get_hand_size(hand_landmarks.landmark)
                    
                    if gesture == "OPEN":
                        print(f"OPEN HAND - Size: {hand_size:.6f}")
                        self.last_measurement_time = time.time()
            
            cv2.imshow('Hand Ligament Analyzer', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        self.cap.release()
        cv2.destroyAllWindows()

def main():
    analyzer = LigamentAnalyzer()
    analyzer.run()

if __name__ == '__main__':
    main()
```

### Usage

```bash
# On ALFRIDROS:
DISPLAY=:1 python3 ~/ligament.py

# Output:
# - Real-time hand skeleton overlay
# - Bone distances printed when open hand detected
# - Press Q to exit

# Used for:
# 1. Calibrating hand size reference
# 2. Debugging hand detection
# 3. Analyzing bone structure
```

---

## ğŸ’» Application 2: butler_camera_display.py

**Location**: `src/butler_camera/butler_camera_display.py`  
**Purpose**: Real-time gesture visualization with colored borders  
**Platform**: ALFRIDROS (Pi5)

```python
#!/usr/bin/env python3

import cv2
import mediapipe as mp

class GestureDisplay:
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3
        )
        
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        self.gesture_buffer = []
        self.buffer_size = 6
        self.gesture_threshold = 4
    
    def count_extended_fingers(self, landmarks):
        """Count how many fingers are extended"""
        extended_count = 0
        
        if landmarks[4].x < landmarks[3].x:
            extended_count += 1
        
        for finger_tip in [8, 12, 16, 20]:
            if landmarks[finger_tip].y < landmarks[finger_tip - 2].y:
                extended_count += 1
        
        return extended_count
    
    def detect_gesture(self, landmarks):
        """Detect gesture: OPEN HAND or FIST"""
        return "OPEN" if self.count_extended_fingers(landmarks) >= 4 else "FIST"
    
    def get_buffered_gesture(self, current_gesture):
        """Use gesture buffering for stability"""
        self.gesture_buffer.append(current_gesture)
        
        if len(self.gesture_buffer) > self.buffer_size:
            self.gesture_buffer.pop(0)
        
        if len(self.gesture_buffer) >= self.gesture_threshold:
            open_count = self.gesture_buffer.count("OPEN")
            if open_count >= self.gesture_threshold:
                return "OPEN"
            elif self.gesture_buffer.count("FIST") >= self.gesture_threshold:
                return "FIST"
        
        return None
    
    def run(self):
        """Main loop for gesture display"""
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_frame)
            
            border_color = (0, 0, 0)
            gesture_text = "NO HAND"
            
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    gesture = self.detect_gesture(hand_landmarks.landmark)
                    buffered_gesture = self.get_buffered_gesture(gesture)
                    
                    if buffered_gesture == "OPEN":
                        border_color = (0, 255, 0)  # Green
                        gesture_text = "YES - OPEN HAND"
                    elif buffered_gesture == "FIST":
                        border_color = (0, 0, 255)  # Red
                        gesture_text = "NO - FIST"
            
            thickness = 5
            cv2.rectangle(frame, (thickness, thickness), 
                         (1280-thickness, 960-thickness), border_color, thickness)
            cv2.putText(frame, gesture_text, (50, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 2, border_color, 3)
            
            cv2.imshow('BUTLER Gesture Detection', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        self.cap.release()
        cv2.destroyAllWindows()

def main():
    display = GestureDisplay()
    display.run()

if __name__ == '__main__':
    main()
```

### Usage

```bash
# On ALFRIDROS Terminal 8:
DISPLAY=:1 python3 ~/butler_camera_display.py

# Output:
# - Real-time video feed
# - GREEN border + "YES - OPEN HAND" = gesture detected
# - RED border + "NO - FIST" = fist detected
# - No border = no hand detected

# Features:
# - 4/6 frame buffering (stability)
# - Large, easy-to-read status text
# - Color-coded for quick visual feedback
```

---

## ğŸ¯ Integration with ROS2

### Connected to Distance Navigator

The hand detection feeds into the **distance navigation system**:

```
ligament.py/butler_camera_display.py
  (detects hand, measures size)
  â†“
ROS2 topic: /hand_detected
ROS2 topic: /hand_size
  â†“
ligament_distance_navigator.py
  (calculates distance, sends movement commands)
  â†“
ROS2 topic: /cmd_vel
  â†“
Motor Control (ALFRIDCL)
  â†“
Robot Movement
```

See [LIGAMENT_NAVIGATOR_ENHANCED.md](LIGAMENT_NAVIGATOR_ENHANCED.md) for integration details.

---

## ğŸ“Š Performance Optimization

### FPS Optimization

```bash
# If getting <10 FPS:

# 1. Install Hailo-8 (3-4x improvement)
# See [AI_HAT_ENHANCED.md]

# 2. Reduce resolution:
self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# 3. Skip frames:
# Only process every 2nd frame in loop

# 4. Disable drawing in production:
# Remove cv2.imshow() calls
```

### CPU Optimization

```bash
# Monitor CPU usage:
top -p $(pidof python3)

# If >60% usage:

# Reduce max hands:
max_num_hands=1  # already doing this

# Reduce detection confidence:
min_detection_confidence=0.5  # from 0.3

# Run on dedicated CPU core:
taskset -c 0 python3 ~/butler_camera_display.py
```

---

## ğŸ” Troubleshooting

### Hand Not Detected

```bash
# Checklist:
1. Check lighting (minimum 200 lux)
2. Camera pointing at hand
3. Hand fully visible in frame
4. Hand in natural position (not twisted)

# Increase detection sensitivity:
min_detection_confidence=0.2  # from 0.3

# Test directly:
python3 ~/ligament.py
# If hand still not detected, hardware issue
```

### Low FPS

```bash
# Check CPU temperature:
vcgencmd measure_temp
# If >75Â°C, add heatsink

# Check background processes:
ps aux | grep python3

# Reduce concurrent tasks:
# Don't run gesture + SLAM + Nav2 together initially
```

### VNC Display Not Working

```bash
# Check DISPLAY variable:
echo $DISPLAY
# Should be ":1"

# If not set:
export DISPLAY=:1
python3 ~/butler_camera_display.py

# Restart VNC if needed:
vncserver -kill :1
vncserver :1 -geometry 1024x768
```

---

## ğŸ–¥ï¸ Tiger VNC Remote Display Access

### Why Use Tiger VNC?

**Direct advantage over SSH:**
- âœ… See GUI applications (OpenCV windows, gesture visualization)
- âœ… Real-time video feed display
- âœ… Interactive desktop environment
- âœ… Works from Mac, Windows, Linux

**VS SSH Terminal:**
- âŒ SSH can't display GUI apps
- âŒ No way to see camera output
- âŒ Limited to command-line only

### Setup Tiger VNC (One Time)

**On ALFRIDROS (Pi5):**

```bash
# Install Tiger VNC and desktop environment:
sudo apt-get install -y tigervnc-server xfce4 xfce4-goodies

# Start VNC server:
vncserver :1 -geometry 1024x768 -depth 24

# When prompted:
# Enter VNC password (different from system password)
# Confirm password
# View-only password? â†’ No

# Kill and restart to apply settings:
vncserver -kill :1
vncserver :1 -geometry 1024x768 -depth 24

# Verify it's running:
ps aux | grep vncserver
# Should show vncserver process
```

### Connect from Your Mac/PC

**Step 1: Download VNC Viewer**

Visit: [RealVNC - VNC Viewer](https://www.realvnc.com/en/connect/download/viewer/)

- ğŸ **macOS**: Download `.dmg` file â†’ Open â†’ Drag to Applications
- ğŸªŸ **Windows**: Download `.exe` â†’ Run installer
- ğŸ§ **Linux**: `sudo apt-get install tigervnc-viewer`

**Step 2: Connect to ALFRIDROS**

```bash
# Option 1: Open VNC Viewer app
# Address bar: ALFRIDROS:5901
# Enter VNC password
# Click "Connect"

# Option 2: Command line (macOS/Linux):
vncviewer ALFRIDROS:5901
# Enter password when prompted

# Option 3: Use IP address instead:
vncviewer 192.168.1.100:5901
# (replace with actual IP from: ssh ubuntu@ALFRIDROS; hostname -I)
```

**Step 3: You Should See**
- XFCE desktop environment
- Taskbar at top/bottom
- Ready to launch applications!

### Run Hand Gesture Display via VNC

**Now in your VNC window, open a terminal:**

```bash
# Right-click on desktop â†’ Open Terminal

# Set DISPLAY variable:
export DISPLAY=:1

# Run gesture display:
python3 ~/butler_camera_display.py

# You should see:
# - Camera feed from USB webcam
# - Real-time hand detection
# - GREEN border when hand OPEN
# - RED border when hand FIST
# - Large status text
```

### VNC Display Tips

```bash
# For better performance, reduce window size:
vncserver :1 -geometry 800x600 -depth 16

# For maximum quality (slower):
vncserver :1 -geometry 1280x960 -depth 24

# Kill VNC cleanly:
vncserver -kill :1

# Check VNC logs:
cat ~/.vnc/ALFRIDROS:1.log

# If VNC port already in use:
# Kill any existing VNC:
pkill -f vncserver
# Then restart
```

### Troubleshooting VNC

**Can't connect:**

```bash
# 1. Verify VNC is running on Pi:
ssh ubuntu@ALFRIDROS
ps aux | grep vncserver
# If no process, restart:
vncserver :1 -geometry 1024x768 -depth 24

# 2. Check firewall:
sudo ufw status
# If active, allow VNC:
sudo ufw allow 5901/tcp

# 3. Use IP instead of hostname:
# Get IP: ssh ubuntu@ALFRIDROS; hostname -I
vncviewer 192.168.1.100:5901

# 4. Try different VNC port:
vncserver :2 -geometry 1024x768 -depth 24
vncviewer ALFRIDROS:5902
```

**VNC window black/frozen:**

```bash
# On ALFRIDROS, restart X server:
vncserver -kill :1
sleep 2
vncserver :1 -geometry 1024x768 -depth 24

# On your Mac/PC:
# Disconnect and reconnect VNC Viewer
```

**Want to run gesture display at VNC startup:**

```bash
# Edit ~/.vnc/xstartup on ALFRIDROS:
nano ~/.vnc/xstartup

# Add at end before "exec startxfce4":
python3 ~/butler_camera_display.py &

# Save and restart VNC:
vncserver -kill :1
vncserver :1 -geometry 1024x768 -depth 24
```

---

## ğŸ”— Full VNC + Gesture Workflow

```bash
# On your Mac/PC:
# 1. Open VNC Viewer
# 2. Connect to: ALFRIDROS:5901
# 3. Enter VNC password
# 4. In VNC window, open terminal
# 5. Run: export DISPLAY=:1
# 6. Run: python3 ~/butler_camera_display.py
# 7. Watch gesture detection in real-time!

# Alternative: SSH tunnel (if firewall blocks 5901):
ssh -L 5901:localhost:5901 ubuntu@ALFRIDROS
# Then VNC to: localhost:5901
```

---

## âœ… Quick Test Procedure

```bash
# 1. Check camera works:
python3 << 'EOF'
import cv2
cap = cv2.VideoCapture(0)
ret, frame = cap.read()
print(f"Camera: {ret}, Shape: {frame.shape}")
cap.release()
EOF

# 2. Test ligament.py:
# DISPLAY=:1 python3 ~/ligament.py
# Show open hand
# Should print: "OPEN HAND - Size: X.XXXXXX"

# 3. Test butler_camera_display.py:
# DISPLAY=:1 python3 ~/butler_camera_display.py
# Should show green/red border based on gesture
```

---

## ğŸš€ Next Steps

- **Want distance control?** â†’ See [LIGAMENT_NAVIGATOR_ENHANCED.md](LIGAMENT_NAVIGATOR_ENHANCED.md)
- **Full 8-terminal setup?** â†’ See [QUICKSTART_ENHANCED.md](QUICKSTART_ENHANCED.md)
- **Performance issues?** â†’ See [AI_HAT_ENHANCED.md](AI_HAT_ENHANCED.md)
- **Stuck?** â†’ See [TROUBLESHOOTING_ENHANCED.md](TROUBLESHOOTING_ENHANCED.md)

---

**Last Updated**: January 31, 2026  
**Version**: 2.0 (January 2026 Production Release)
